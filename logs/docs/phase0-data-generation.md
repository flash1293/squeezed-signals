# Phase 0: Log Data Generation - Real-World Datasets

Phase 0 establishes the foundation by sourcing **real-world log data** from the LogHub repository, ensuring our compression techniques are tested against genuine production log patterns.

## ðŸ“‹ Data Source Overview

The project uses the [LogHub dataset repository](https://github.com/logpai/loghub), which provides diverse system logs from various production environments:

- **Apache**: Web server error logs with HTTP request patterns
- **HDFS**: Hadoop distributed file system logs with cluster operations
- **OpenSSH**: SSH server authentication and connection logs
- **Linux**: System-level logs with kernel and service messages
- **OpenStack**: Cloud infrastructure logs with API operations

## ðŸŽ¯ Why Real Data Matters

Unlike synthetic data, real-world logs have characteristics that significantly impact compression:

### Natural Log Patterns
- **Template repetition**: Similar events generate identical log structures
- **Temporal clustering**: Related events occur in bursts
- **Variable locality**: Similar values appear together (IP ranges, sequential IDs)
- **Hierarchical structure**: Logs follow consistent formatting conventions

### Production Characteristics
- **Variable sparsity**: Not all fields present in every log line
- **Format inconsistencies**: Mixed date formats, irregular spacing
- **Real entropy**: Actual UUIDs, hashes, and random identifiers
- **Natural distributions**: Realistic timestamp gaps and value patterns

## ðŸ“Š Dataset Configurations

The project provides three dataset sizes for different testing scenarios:

### Small Dataset: Apache Logs
```
Dataset: Apache web server error log
Source: LogHub Apache_2k.log
Lines: ~2,000
Size: ~0.5 MB
Characteristics:
  - HTTP error messages with status codes
  - Client IP addresses and request paths
  - Timestamp patterns: [Day Mon DD HH:MM:SS YYYY]
  - Common templates: Connection errors, file not found, access denied
```

**Example logs:**
```
[Thu Jun 09 06:07:04 2005] [error] [client 64.242.88.10] File does not exist: /var/www/html/robots.txt
[Thu Jun 09 06:07:05 2005] [error] [client 64.242.88.10] File does not exist: /var/www/html/favicon.ico
```

### Big Dataset: HDFS Logs
```
Dataset: Hadoop HDFS distributed file system log
Source: LogHub HDFS_2k.log (expandable to full 11M+ lines)
Lines: ~2,000 (sample) / ~11,175,629 (full)
Size: ~1.0 MB (sample) / ~1.5 GB (full)
Characteristics:
  - Block operations and replication events
  - DataNode and NameNode communication
  - Timestamp format: YYMMDD HHMMSS
  - Block IDs: blk_NNNNNNNNNN format
  - Rich variable patterns: IPs, paths, identifiers
```

**Example logs:**
```
081109 203615 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.251.43.191:54106 dest: /10.251.43.191:50010
081109 203615 145 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-1608999687919862906 terminating
```

### Huge Dataset: OpenSSH Logs
```
Dataset: OpenSSH server authentication and connection log
Source: LogHub OpenSSH_2k.log (expandable to full 655k+ lines)
Lines: ~2,000 (sample) / ~655,146 (full)
Size: ~1.0 MB (sample) / ~80 MB (full)
Characteristics:
  - SSH authentication attempts (success/failure)
  - Connection lifecycle events
  - User and host information
  - Syslog timestamp format: Mon DD HH:MM:SS
  - Security-related patterns: passwords, keys, authentication methods
```

**Example logs:**
```
Dec 10 06:55:46 LabSZ sshd[24200]: reverse mapping checking getaddrinfo for ns.marryaldkfaczcz.com failed - POSSIBLE BREAK-IN ATTEMPT!
Dec 10 06:55:46 LabSZ sshd[24200]: Failed password for root from 218.188.2.4 port 52825 ssh2
```

## ðŸ” Log Structure Analysis

The data generator performs comprehensive analysis of each dataset:

### Template Discovery
- **Unique template patterns**: Identify static vs. variable parts
- **Template frequency**: Most common log patterns
- **Variable type detection**: Timestamps, IPs, numbers, paths, identifiers

### Variable Characteristics
```
Variable Types Detected:
  - Timestamps: ISO8601, Unix epoch, Syslog format, custom formats
  - IP Addresses: IPv4 (can extend to IPv6)
  - File Paths: Absolute paths, relative paths
  - Identifiers: Block IDs, session IDs, process IDs
  - Numbers: Integers, ports, sizes, error codes
  - Hex Strings: Memory addresses, checksums
  - UUIDs: Session identifiers, transaction IDs
  - Text: Error messages, usernames, hostnames
```

### Statistical Profile
```
Analysis Output:
  Total lines: 2,000
  File size: 567,890 bytes (554 KB)
  Average line length: 283.9 characters
  Line length range: 45-892 characters
  Timestamps detected: 2,000 (100%)
  IP addresses detected: 1,847 (92.4%)
  Log levels found: INFO, ERROR, WARN, DEBUG
  Unique prefixes: 18
```

## ðŸ’¾ Data Processing Pipeline

### 1. Download and Cache
```python
# Intelligent caching avoids repeated downloads
cache_dir = Path("cache")
cached_file = cache_dir / "Apache.log"

if cached_file.exists():
    print(f"Using cached dataset: {cached_file}")
else:
    download_from_loghub(url, cached_file)
```

### 2. Dataset Trimming (Optional)
```python
# For full datasets, trim to target size for testing
if full_dataset_size > target_size:
    trimmed_file = trim_dataset_to_size(
        input_file=full_dataset,
        target_size_mb=10  # Trim to 10MB for testing
    )
```

### 3. Format Standardization
```python
# Ensure consistent line format
with open(output_file, 'w') as out:
    for line in input_file:
        line = line.strip()
        if line:  # Skip empty lines
            out.write(line + '\n')
```

### 4. Metadata Generation
```python
metadata = {
    'dataset': 'Apache',
    'source_url': 'https://github.com/logpai/loghub',
    'lines_processed': 2000,
    'file_size_bytes': 567890,
    'analysis': {
        'avg_line_length': 283.9,
        'timestamps_detected': 2000,
        'ip_addresses_detected': 1847,
        'log_levels': ['INFO', 'ERROR', 'WARN']
    }
}
```

## ðŸŽ¨ Sample Log Patterns

### Apache: Web Server Errors
```
Pattern: [<TIMESTAMP>] [<LEVEL>] [client <IP>] <MESSAGE> <PATH>
Variables: Timestamp, IP address, error message, file path
Frequency: Very repetitive templates (high compression potential)
```

### HDFS: Distributed File System
```
Pattern: <TIMESTAMP> <NUM> <LEVEL> <CLASS>: <ACTION> block <IDENTIFIER> ...
Variables: Timestamp, thread ID, block ID, IP addresses, paths
Frequency: Moderate template variety (good compression potential)
```

### OpenSSH: Authentication Events
```
Pattern: <TIMESTAMP> <HOST> sshd[<NUM>]: <AUTH_MESSAGE> for <USER> from <IP> ...
Variables: Timestamp, host, PID, username, IP address
Frequency: High template repetition with clustered values
```

## ðŸ“ˆ Compression Opportunity Analysis

### Template Repetition
```
Top Templates by Frequency:
  1. "INFO: User '' logged in from IP " - 347 occurrences (17.4%)
  2. "ERROR: Failed to process block blk_" - 213 occurrences (10.7%)
  3. "WARN: Connection timeout from " - 189 occurrences (9.5%)
```

**Compression potential**: 40-60x from template extraction alone

### Variable Clustering
```
IP Address Analysis:
  - 64.242.88.10: 423 occurrences
  - 10.251.43.191: 387 occurrences
  - Similar subnet patterns indicate clustering potential
```

**Compression potential**: Additional 2-3x from variable encoding

### Temporal Locality
```
Timestamp Analysis:
  - 94% of timestamps within 1-second increments
  - Clear burst patterns (10-100 logs in rapid succession)
  - Natural ordering creates excellent delta encoding potential
```

**Compression potential**: 10-20x from timestamp delta encoding

## ðŸ”§ Configuration Options

### Dataset Size Selection
```bash
# Generate small dataset (~0.5MB)
python 00_generate_data.py --size small

# Generate big dataset (~1MB sample, expandable)
python 00_generate_data.py --size big

# Generate huge dataset (~1MB sample, expandable)
python 00_generate_data.py --size huge
```

### Full Dataset Mode
```bash
# Use full datasets instead of 2k samples
python 00_generate_data.py --size big
# Default: Automatically downloads and uses full datasets

# Force sample-only mode (2k lines)
python 00_generate_data.py --size big --sample-only
```

## ðŸ“Š Output Files

```
output/
  logs_small.log                      # Raw log data
  phase0_logs_metadata_small.json     # Analysis metadata
```

**Metadata contents:**
```json
{
  "phase": "Phase 0 - Log Data Generation",
  "dataset": "Apache",
  "source_url": "https://raw.githubusercontent.com/logpai/loghub/...",
  "lines_written": 2000,
  "output_file_size": 567890,
  "analysis": {
    "total_lines": 2000,
    "avg_line_length": 283.9,
    "timestamps_detected": 2000,
    "ip_addresses_detected": 1847,
    "log_levels": ["INFO", "ERROR", "WARN"],
    "sample_lines": ["...", "...", "..."]
  }
}
```

## ðŸŽ¯ Why This Matters

Using real-world log data ensures:

1. **Realistic compression ratios**: Results applicable to production systems
2. **Pattern diversity**: Tests handle various log formats and structures
3. **Edge cases**: Real data includes malformed lines, encoding issues
4. **Benchmark validity**: Industry-standard datasets enable comparisons
5. **Production readiness**: Techniques proven on actual log patterns

The baseline dataset establishes the **uncompressed size** target that subsequent phases will optimize, proving compression effectiveness on genuine production log data.

## ðŸ”„ Next Steps

Phase 0 provides the raw log data baseline. The next phases progressively apply compression techniques:

- **Phase 1**: Plain text baseline (1x - no compression)
- **Phase 2**: Zstd compression (3-5x)
- **Phase 3**: Template extraction (10-20x)
- **Phase 4**: Advanced variable encoding (25-50x)
- **Phase 5**: Smart row ordering (50-100x)

The real-world nature of this data ensures that all compression improvements are grounded in practical, production-applicable scenarios.
